{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CazISR8X_HUG"
   },
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions of Linear regression\n",
    "\n",
    "The Linear regression has the following assumptions:\n",
    "1. Linearity\n",
    "2. Homoscedasticity\n",
    "3. Multivarable Normality\n",
    "4. Independence of Erros\n",
    "5. Lack of multicollinearity\n",
    "\n",
    "Always remember to recheck these assumptions before designing the linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition of Multiple Linear Regression : Dummy Variables\n",
    "\n",
    "**Dummy Variables**\n",
    "\n",
    "There are two types of varialbes: Categorical variables (eg. Country: Germany, USA, UK) and numerical variables (Eg. Profit: 100, 200). \n",
    "\n",
    "We can only work with numerical variables and hence the categorical variables need to be converted to the numerical values. This can be done by introducing dummy variables. This is explained in the encoding step.\n",
    "\n",
    "**Dummy variable Trap**\n",
    "\n",
    "It is not good to include all the dummy columns in the model. For example: If we have a categorial varialbe with two countires values, and after encoding them the one dummy column itself can define the state of the other column. (because a value of 1 represents the true case and value of 0 represents the false case or the true case for the second variable)  \n",
    "\n",
    "In a linear regression, consider a case with two dummy variables, D1 & D2.\n",
    "\n",
    "*D2 = 1- D1*  \n",
    "\n",
    "So if we introduce both variables into our model, it fails to distinguish the effect of both. This is called as dummy variable trap. Because of this, only one dummy variable is introduced into any model with two values of categorical variables.\n",
    "\n",
    "There are no bias added into the system only by introducing one variable in this case. The vale of b0 in the multiple linear regression equation balances this bias effect. \n",
    "\n",
    "**As a general rule: If the Term b0 is available in the multiple linear equation, then always omit one dummy variable from the categorical variable class**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P-Values & Statistical Significance\n",
    "\n",
    "Intuition for Statistical Significance.\n",
    "Consider the case of tossing a coin. We have two Hypothesises here. \n",
    "\n",
    "1. A null hypothesis (H_0): In one universe, the coin is a fair coin (so we have equal chances of getting a head or tail)\n",
    "2. An alternative Hypothesis (H_A): In the other universe, the coin is not a fair coin. (So there is a chance to get either head or tail in most of the cases)\n",
    "\n",
    "If we live in a universe where the null hypothesis is true, we can perform the experiments and see the probabilities to get an expected result. \n",
    "\n",
    "For example: \n",
    "Probability to get a tail in first toss : 0.5\n",
    "Probability to get a tail again in the second toss : 0.25\n",
    "Probability to get a tail again in the third toss : 0.12\n",
    "Probability to get a tail again in the fourth toss : 0.06\n",
    "\n",
    "(The probability of getting a tail in the 4th toss consicutively is only 0.0625, but having our null hypothesis as true, if we get tail again in the 5th toss, the probaility is really really less (only 3%), then we have to question our assumption we made in the null hypothesis; that is the coin is fair or not. \n",
    "\n",
    "Which means, if we get a tail in the 7th toss consicutively, there is a chance that our null hypothesis is wrong and the coin is not a fair one. This is a natural feeling considering the statistics.)\n",
    "\n",
    "The P-values (Probability values) will reduce gradually considering this scenario of tossing a fair coin. \n",
    "\n",
    "Consider the Alternate hypothesis H_A, where we have a coin with both tail faces. Then we will get tail in all the tosses. And the P-value will be 100% in all the cases.\n",
    "\n",
    "\n",
    "Here is the imporance of the Statistical significance. If we consider our case of null hypothesis is true and we set a boundary P-Value = 0.05 (5%), and if we get results less than the limit P-value, then we can question the validity of our null hypothesis. Hence we can reject the null hypothesis. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-step to the Model Building\n",
    "\n",
    "Why we should filter the independent variables while designing the model?\n",
    "\n",
    "1. Garbage in == Garbage out : If we feed the model with too much, even unnecessary data, we may end up in a Garbage model and which will give us bad results. So we have to select only the significant independent variable in the model.\n",
    "\n",
    "2. The model shall be mathematically explainable considering the dependancy of the parameters.\n",
    "\n",
    "So, how we will determine which parameters to eliminate in the models?\n",
    "\n",
    "**5 Methods of Building models**\n",
    "1. All-in\n",
    "2. Backward Elimination\n",
    "3. Forward selection\n",
    "4. Bidirectional Elimination\n",
    "5. Score Comparison\n",
    "\n",
    "(The 2, 3 & 4 are called together sometimes as Stepwise Regression)\n",
    "\n",
    "\n",
    "1. **All In**: Consider all the variables\n",
    "Will be done if we know the scenario or someone gives you all the variables and asks you to build the model out of it.\n",
    "    + Prior Knowledge\n",
    "    + You have to do (someone gives you a task to build model \n",
    "    + A framework in the system which advice you to use all the variables.\n",
    "    + You are preparing for the Backward Elimination step.\n",
    "    \n",
    "\n",
    "2. **Backward Elimination**: Eliminate each variable at each step based on a SL\n",
    "    + Step 1: Select a significance level (P-Value) to stay in the model (eg: SL= 0.05/5%)\n",
    "    + Step 2: Fit the full model with all possible predictors\n",
    "    + Step 3: Consider the Variable/ predictor with the **Highest** P-Value. \n",
    "        If P > SL go to step 4\n",
    "        otherwise go to FIN\n",
    "    + Step 4: Remove the variable\n",
    "    + Step 5: Fit the model without this variable\n",
    "    + Step 6: Go to step 3 and check for the highest P-value variable.\n",
    "    \n",
    "    Model is finished when the highest P-value is no longer greater than the SL.\n",
    "\n",
    "3. **Forward Selection**:\n",
    "    + step 1: Select a significance level to enter the model (eg SL = 0.05)\n",
    "    + Step 2: Fit all simple regression models, **y ~ Xn**,  Select the one variable with the lowest P-Value\n",
    "    + Step 3: Keep this variable and fit all possible models with one extra predictor added to the one(s) you already habe\n",
    "    + Step 4: Consider the predictor with the **Lowest** P-Value. If **P < SL**, go to step 3, otherwise go to FIN\n",
    "    + FIN: if P is no longer less than SL, Keep the **Previous** Model. \n",
    "    \n",
    "    \n",
    "4. **Bidirectional Elimination**\n",
    "    + Step 1: Select a significance level to enter and stay in the model (eg: SLEnter = 0.05, SLStay = 0.05)\n",
    "    + Step 2: Peform the next step of Forward selection (new variable must have P < SLEnter to enter)\n",
    "    + Step 3: Perform ALL the steps of Backward Elimination (Old varialbes must have P < SLStay to stay)\n",
    "    + Step 4: Go to step 2, Step 3 iteratively till No new variables can enter or no old variables can exit\n",
    "    + FIN: Finally the model is ready. \n",
    "    \n",
    "    \n",
    "5. **All Possible Models**: Score comparison\n",
    "\n",
    "    + Step 1: Select a criterion of Goodness of Fit (eg: Akaike criterion)\n",
    "    + Step 2: Construct all possible regression models: 2^N-1 total combinations\n",
    "    + Step 3: Select the one with the best criterion\n",
    "    + FIN : Model is ready\n",
    "    \n",
    "    Example: if you have 10 columns, there will be 1023 models. So it is resource consuming approach\n",
    "\n",
    "In this tutorial, we have used the Backward Elimination approach (because it is faster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pOyqYHTk_Q57"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T_YHJjnD_Tja"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vgC61-ah_WIz"
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UrxyEKGn_ez7"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('50_Startups.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R&amp;D Spend</th>\n",
       "      <th>Administration</th>\n",
       "      <th>Marketing Spend</th>\n",
       "      <th>State</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165349.20</td>\n",
       "      <td>136897.80</td>\n",
       "      <td>471784.10</td>\n",
       "      <td>New York</td>\n",
       "      <td>192261.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>162597.70</td>\n",
       "      <td>151377.59</td>\n",
       "      <td>443898.53</td>\n",
       "      <td>California</td>\n",
       "      <td>191792.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "      <td>Florida</td>\n",
       "      <td>191050.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144372.41</td>\n",
       "      <td>118671.85</td>\n",
       "      <td>383199.62</td>\n",
       "      <td>New York</td>\n",
       "      <td>182901.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142107.34</td>\n",
       "      <td>91391.77</td>\n",
       "      <td>366168.42</td>\n",
       "      <td>Florida</td>\n",
       "      <td>166187.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   R&D Spend  Administration  Marketing Spend       State     Profit\n",
       "0  165349.20       136897.80        471784.10    New York  192261.83\n",
       "1  162597.70       151377.59        443898.53  California  191792.06\n",
       "2  153441.51       101145.55        407934.54     Florida  191050.39\n",
       "3  144372.41       118671.85        383199.62    New York  182901.99\n",
       "4  142107.34        91391.77        366168.42     Florida  166187.94"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 857
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 552,
     "status": "ok",
     "timestamp": 1586353652778,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "GOB3QhV9B5kD",
    "outputId": "4a05377a-2db2-43fc-b824-a0710448baee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[165349.2 136897.8 471784.1 'New York']\n",
      " [162597.7 151377.59 443898.53 'California']\n",
      " [153441.51 101145.55 407934.54 'Florida']\n",
      " [144372.41 118671.85 383199.62 'New York']\n",
      " [142107.34 91391.77 366168.42 'Florida']\n",
      " [131876.9 99814.71 362861.36 'New York']\n",
      " [134615.46 147198.87 127716.82 'California']\n",
      " [130298.13 145530.06 323876.68 'Florida']\n",
      " [120542.52 148718.95 311613.29 'New York']\n",
      " [123334.88 108679.17 304981.62 'California']\n",
      " [101913.08 110594.11 229160.95 'Florida']\n",
      " [100671.96 91790.61 249744.55 'California']\n",
      " [93863.75 127320.38 249839.44 'Florida']\n",
      " [91992.39 135495.07 252664.93 'California']\n",
      " [119943.24 156547.42 256512.92 'Florida']\n",
      " [114523.61 122616.84 261776.23 'New York']\n",
      " [78013.11 121597.55 264346.06 'California']\n",
      " [94657.16 145077.58 282574.31 'New York']\n",
      " [91749.16 114175.79 294919.57 'Florida']\n",
      " [86419.7 153514.11 0.0 'New York']\n",
      " [76253.86 113867.3 298664.47 'California']\n",
      " [78389.47 153773.43 299737.29 'New York']\n",
      " [73994.56 122782.75 303319.26 'Florida']\n",
      " [67532.53 105751.03 304768.73 'Florida']\n",
      " [77044.01 99281.34 140574.81 'New York']\n",
      " [64664.71 139553.16 137962.62 'California']\n",
      " [75328.87 144135.98 134050.07 'Florida']\n",
      " [72107.6 127864.55 353183.81 'New York']\n",
      " [66051.52 182645.56 118148.2 'Florida']\n",
      " [65605.48 153032.06 107138.38 'New York']\n",
      " [61994.48 115641.28 91131.24 'Florida']\n",
      " [61136.38 152701.92 88218.23 'New York']\n",
      " [63408.86 129219.61 46085.25 'California']\n",
      " [55493.95 103057.49 214634.81 'Florida']\n",
      " [46426.07 157693.92 210797.67 'California']\n",
      " [46014.02 85047.44 205517.64 'New York']\n",
      " [28663.76 127056.21 201126.82 'Florida']\n",
      " [44069.95 51283.14 197029.42 'California']\n",
      " [20229.59 65947.93 185265.1 'New York']\n",
      " [38558.51 82982.09 174999.3 'California']\n",
      " [28754.33 118546.05 172795.67 'California']\n",
      " [27892.92 84710.77 164470.71 'Florida']\n",
      " [23640.93 96189.63 148001.11 'California']\n",
      " [15505.73 127382.3 35534.17 'New York']\n",
      " [22177.74 154806.14 28334.72 'California']\n",
      " [1000.23 124153.04 1903.93 'New York']\n",
      " [1315.46 115816.21 297114.46 'Florida']\n",
      " [0.0 135426.92 0.0 'California']\n",
      " [542.05 51743.15 0.0 'New York']\n",
      " [0.0 116983.8 45173.06 'California']]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VadrvE7s_lS9"
   },
   "source": [
    "## Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have one categorical data in the dataset, hence we have to perform one hot encoding for this data column.\n",
    "The sklearn tool OneHotEncoder has been used for this approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wV3fD1mbAvsh"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 857
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 616,
     "status": "ok",
     "timestamp": 1586353657759,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "4ym3HdYeCGYG",
    "outputId": "ce09e670-cf06-4a1c-f5b0-89422aae0496",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 165349.2 136897.8 471784.1]\n",
      " [1.0 0.0 0.0 162597.7 151377.59 443898.53]\n",
      " [0.0 1.0 0.0 153441.51 101145.55 407934.54]\n",
      " [0.0 0.0 1.0 144372.41 118671.85 383199.62]\n",
      " [0.0 1.0 0.0 142107.34 91391.77 366168.42]\n",
      " [0.0 0.0 1.0 131876.9 99814.71 362861.36]\n",
      " [1.0 0.0 0.0 134615.46 147198.87 127716.82]\n",
      " [0.0 1.0 0.0 130298.13 145530.06 323876.68]\n",
      " [0.0 0.0 1.0 120542.52 148718.95 311613.29]\n",
      " [1.0 0.0 0.0 123334.88 108679.17 304981.62]\n",
      " [0.0 1.0 0.0 101913.08 110594.11 229160.95]\n",
      " [1.0 0.0 0.0 100671.96 91790.61 249744.55]\n",
      " [0.0 1.0 0.0 93863.75 127320.38 249839.44]\n",
      " [1.0 0.0 0.0 91992.39 135495.07 252664.93]\n",
      " [0.0 1.0 0.0 119943.24 156547.42 256512.92]\n",
      " [0.0 0.0 1.0 114523.61 122616.84 261776.23]\n",
      " [1.0 0.0 0.0 78013.11 121597.55 264346.06]\n",
      " [0.0 0.0 1.0 94657.16 145077.58 282574.31]\n",
      " [0.0 1.0 0.0 91749.16 114175.79 294919.57]\n",
      " [0.0 0.0 1.0 86419.7 153514.11 0.0]\n",
      " [1.0 0.0 0.0 76253.86 113867.3 298664.47]\n",
      " [0.0 0.0 1.0 78389.47 153773.43 299737.29]\n",
      " [0.0 1.0 0.0 73994.56 122782.75 303319.26]\n",
      " [0.0 1.0 0.0 67532.53 105751.03 304768.73]\n",
      " [0.0 0.0 1.0 77044.01 99281.34 140574.81]\n",
      " [1.0 0.0 0.0 64664.71 139553.16 137962.62]\n",
      " [0.0 1.0 0.0 75328.87 144135.98 134050.07]\n",
      " [0.0 0.0 1.0 72107.6 127864.55 353183.81]\n",
      " [0.0 1.0 0.0 66051.52 182645.56 118148.2]\n",
      " [0.0 0.0 1.0 65605.48 153032.06 107138.38]\n",
      " [0.0 1.0 0.0 61994.48 115641.28 91131.24]\n",
      " [0.0 0.0 1.0 61136.38 152701.92 88218.23]\n",
      " [1.0 0.0 0.0 63408.86 129219.61 46085.25]\n",
      " [0.0 1.0 0.0 55493.95 103057.49 214634.81]\n",
      " [1.0 0.0 0.0 46426.07 157693.92 210797.67]\n",
      " [0.0 0.0 1.0 46014.02 85047.44 205517.64]\n",
      " [0.0 1.0 0.0 28663.76 127056.21 201126.82]\n",
      " [1.0 0.0 0.0 44069.95 51283.14 197029.42]\n",
      " [0.0 0.0 1.0 20229.59 65947.93 185265.1]\n",
      " [1.0 0.0 0.0 38558.51 82982.09 174999.3]\n",
      " [1.0 0.0 0.0 28754.33 118546.05 172795.67]\n",
      " [0.0 1.0 0.0 27892.92 84710.77 164470.71]\n",
      " [1.0 0.0 0.0 23640.93 96189.63 148001.11]\n",
      " [0.0 0.0 1.0 15505.73 127382.3 35534.17]\n",
      " [1.0 0.0 0.0 22177.74 154806.14 28334.72]\n",
      " [0.0 0.0 1.0 1000.23 124153.04 1903.93]\n",
      " [0.0 1.0 0.0 1315.46 115816.21 297114.46]\n",
      " [1.0 0.0 0.0 0.0 135426.92 0.0]\n",
      " [0.0 0.0 1.0 542.05 51743.15 0.0]\n",
      " [1.0 0.0 0.0 0.0 116983.8 45173.06]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: **In Multiple Linear Regression, there is no need to apply a feature scaling, the coefficients will balance the values**\n",
    "\n",
    "Note: **Do we need to check the linearity assumption? No, if the dataset has no linear relationship, our model will give a bad result. That's all**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WemVnqgeA70k"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kb_v_ae-A-20"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k-McZVsQBINc"
   },
   "source": [
    "## Training the Multiple Linear Regression model on the Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do we need to be aware about the Dummy Variable trap??**\n",
    "\n",
    "No, because the Multiple Linear regression class, we are going to import from sklearn can avaoid the dummy variable trap automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do we have to do Backward elimination**\n",
    "\n",
    "No, the class from the Sklearn will balance the model by itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 757,
     "status": "ok",
     "timestamp": 1586353664008,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "ywPjx0L1BMiD",
    "outputId": "099836bc-4d85-4b4f-a488-093faf02e8cb",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# It is the same class used for the single and multiple linear regression.\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xNkXL1YQBiBT"
   },
   "source": [
    "## Predicting the Test set results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cann't plot a graph as we have many variables, so we simply create two vectors and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 951,
     "status": "ok",
     "timestamp": 1586353666678,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "TQKmwvtdBkyb",
    "outputId": "493436bf-a4ae-4374-ca16-0b0c25d19457"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103015.2  103282.38]\n",
      " [132582.28 144259.4 ]\n",
      " [132447.74 146121.95]\n",
      " [ 71976.1   77798.83]\n",
      " [178537.48 191050.39]\n",
      " [116161.24 105008.31]\n",
      " [ 67851.69  81229.06]\n",
      " [ 98791.73  97483.56]\n",
      " [113969.44 110352.25]\n",
      " [167921.07 166187.94]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = regressor.predict(X_test)\n",
    "np.set_printoptions(precision=2)\n",
    "# Convert the vectors from horizontal to vertical, use the reshape and len functions.\n",
    "# The concatination vertical given a value of 1\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Elimination\n",
    "\n",
    "Backward Elimination is irrelevant in Python, because the Scikit-Learn library automatically takes care of selecting the statistically significant features when training the model to make accurate predictions.\n",
    "\n",
    "However, if you do really want to learn how to manually implement Backward Elimination in Python and identify the most statistically significant features, please find in this link below some old videos I made on how to implement Backward Elimination in Python:\n",
    "\n",
    "https://www.dropbox.com/sh/pknk0g9yu4z06u7/AADSTzieYEMfs1HHxKHt9j1ba?dl=0\n",
    "\n",
    "\n",
    "These are old videos made on Spyder but the dataset and the code are the same as in the previous video lectures of this section on Multiple Linear Regression, except that I had manually removed the first column to avoid the Dummy Variable Trap with this line of code:\n",
    "\n",
    "Avoiding the Dummy Variable Trap\n",
    "\n",
    "X = X[:, 1:]\n",
    "\n",
    "\n",
    "Just keep this for this Backward Elimination implementation, but keep in mind that in general you don't have to remove manually a dummy variable column because Scikit-Learn takes care of it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: How do I use my multiple linear regression model to make a single prediction, for example, the profit of a startup with R&D Spend = 160000, Administration Spend = 130000, Marketing Spend = 300000 and State = California?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predict = [[1.0, 0.0, 0.0, 160000, 130000, 300000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_predict)\n",
    "# gives a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181566.92389385228"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2: How do I get the final regression equation y = b0 + b1 x1 + b2 x2 + ... with the final values of the coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.66e+01 -8.73e+02  7.86e+02  7.73e-01  3.29e-02  3.66e-02]\n",
      "42467.52924854249\n"
     ]
    }
   ],
   "source": [
    "print(regressor.coef_)\n",
    "print(regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the equation of our multiple linear regression model is:\n",
    "\n",
    "**Profit= 86.6× Dummy State1 − 873× Dummy State2 + 786× Dummy State3 + 0.773× R&D Spend + 0.0329× Administration + 0.0366 × Marketing Spend + 42467.53**\n",
    "\n",
    "Important Note: To get these coefficients we called the \"coef_\" and \"intercept_\" attributes from our regressor object. Attributes in Python are different than methods and usually return a simple value or an array of values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPhYhte6t7H4wEK4xPpDWT7",
   "name": "Multiple Linear Regression",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
